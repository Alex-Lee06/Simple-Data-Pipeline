# Simple-Data-Pipeline
This project will walk you through on to set up and configure a node Hadoop Cluster installation and a data pipeline for Big Data on Ubuntu 16.04. You will need to have basic knowledge in Linux, Kafka, Scala, Spark, NoSQL. This walk-through will be for 2 or more clusters project so that you can quickly perform simple installation on the other nodes. 

# Installing Ubuntu 16.04
First, installing Ubuntu to:

To begin Ubuntu install version 16.04 follow the link below.  You will need to know if you can run the 64-bit processor or a 32-bit process.  You can find that information from your system settings.  You will need Bit Torrent for the installation.

Bit Torrent installation for Windows OS:
http://www.bittorrent.com/downloads/win

Bit Torrent installation for MAC OS:
http://www.bittorrent.com/downloads/mac

Link to install Ubuntu 16.04
https://www.ubuntu.com/download/alternative-downloads


Once you already have been instales Ubuntu 16.04 The version requirer for the tools are: 

Java 1.8.0_171
Hadoop 2.7.6
Zookeerper 3.4.10
Kafka 1.0.0
Spark 2.1


