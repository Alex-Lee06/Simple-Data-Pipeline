# Simple-Data-Pipeline
This project will walk you through on creating a data pipeline for Big Data using Hadoop.  You will need to have basic knowledge in Linux, Scala, Spark, NoSQL.  This walk-through will be for 2 or more clusters project.  All computers that is used will have Ubuntu 16-04 OS installed.

# Installing Ubuntu 16.04
First, installing Ubuntu to:

To begin Ubuntu install version 16.04 follow the link below.  You will need to know if you can run the 64-bit processor or a 32-bit process.  You can find that information from your system settings.  You will need Bit Torrent for the installation.

Bit Torrent installation for Windows OS:
http://www.bittorrent.com/downloads/win

Bit Torrent installation for MAC OS:
http://www.bittorrent.com/downloads/mac

Link to install Ubuntu 16.04
https://www.ubuntu.com/download/alternative-downloads

# Following along via Github,

First make sure you have a valid Github account

If not, then you can make an account at https://github.com/

	From terminal:

	Make sure Github is installed: `$ sudo apt-get install git`

	Cloning the repository to your local: `$ git clone https://github.com/Alex-Lee06/Simple-Data-Pipeline.git`

# Checking and installing Java

To run Hadoop (or an Apache Big Data service), YOU WILL NEED JAVA

To check to see if you have Java installed on your system

	From terminal:

	You can check your Java version by running: `$ java -version`

	OR

	You can run: `$ java` or `$ javac` 
	
	if you get a usage message then java is installed.

	
If you do NOT have Java installed then, 

	From terminal:

	Run the command: `$ sudo apt-get install default-jdk`

	And this will download the most recent version of the JDK


